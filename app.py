import streamlit as st
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from llama_index.core import SimpleDirectoryReader
import os
from dotenv import load_dotenv

# è¿è¡Œ: streamlit run app.py

# ======================
# Streamlité¡µé¢é…ç½®ï¼ˆå¯è§†åŒ–åŸºç¡€ï¼‰
# ======================
st.set_page_config(
    page_title="PDFæ™ºèƒ½é—®ç­”åŠ©æ‰‹",  # é¡µé¢æ ‡é¢˜
    page_icon="ğŸ“„",                # é¡µé¢å›¾æ ‡
    layout="wide"                  # å®½å±å¸ƒå±€
)

# é¡µé¢æ ‡é¢˜+è¯´æ˜
st.title("ğŸ“„ RAG PDFæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼ˆé€šä¹‰åƒé—®ç‰ˆï¼‰")
st.divider()

# ======================
# åŠ è½½ç¯å¢ƒå˜é‡+åŸºç¡€é…ç½®
# ======================
load_dotenv()

# é…ç½®é¡¹ï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
EMBEDDING_PATH = "./models/bge-small-zh-v1.5"
CHROMA_DIR = "./chroma_db"
LLM_MODEL = "qwen-plus"
TEMPERATURE = 0.1

# ======================
# ä¾§è¾¹æ ï¼šé…ç½®+PDFä¸Šä¼ 
# ======================
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    # æ‰‹åŠ¨è¾“å…¥APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œè¦†ç›–.envï¼‰
    qwen_api_key = st.text_input("é€šä¹‰åƒé—®APIå¯†é’¥", type="password", value=os.getenv("QWEN_API_KEY", ""))
    qwen_base_url = st.text_input("APIåœ°å€", value=os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"))
    
    st.divider()
    
    # PDFä¸Šä¼ ï¼ˆæ ¸å¿ƒï¼šæ”¯æŒä¸Šä¼ ä»»æ„PDFï¼Œæ›¿ä»£å›ºå®šdemo.pdfï¼‰
    st.header("ğŸ“¤ ä¸Šä¼ PDF")
    uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

# ======================
# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰
# ======================
@st.cache_resource  # ç¼“å­˜èµ„æºï¼Œé¿å…æ¯æ¬¡åˆ·æ–°é‡æ–°åˆå§‹åŒ–
def init_components(uploaded_pdf):
    """åˆå§‹åŒ–Embedding+LLM+å‘é‡åº“"""
    # 1. ä¿å­˜ä¸Šä¼ çš„PDFåˆ°æœ¬åœ°
    pdf_path = "./temp.pdf"
    with open(pdf_path, "wb") as f:
        f.write(uploaded_pdf.getbuffer())
    
    # 2. åˆå§‹åŒ–Embeddingï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_PATH,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    # 3. åˆå§‹åŒ–LLMï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
    llm = ChatOpenAI(
        model=LLM_MODEL,
        temperature=TEMPERATURE,
        api_key=qwen_api_key or os.getenv("QWEN_API_KEY"),
        base_url=qwen_base_url or os.getenv("QWEN_BASE_URL")
    )
    
    # 4. åŠ è½½PDF+æ„å»ºå‘é‡åº“ï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
    llama_docs = SimpleDirectoryReader(input_files=[pdf_path]).load_data()
    docs = [
        Document(page_content=doc.text.strip(), metadata={"page": doc.metadata.get("page_label", "æœªçŸ¥")})
        for doc in llama_docs
    ]
    vector_db = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=CHROMA_DIR
    )
    retriever = vector_db.as_retriever(search_kwargs={"k": 2})
    
    # 5. æ„å»ºRAGé“¾ï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸­æ–‡æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ï¼Œä»…åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œç­”æ¡ˆç®€æ´æ˜äº†ï¼š\n{context}"),
        ("human", "{question}")
    ])
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever

# ======================
# æ ¸å¿ƒé€»è¾‘ï¼šåªæœ‰ä¸Šä¼ PDFåæ‰åˆå§‹åŒ–
# ======================
if uploaded_file:
    # åˆå§‹åŒ–ç»„ä»¶
    with st.spinner("ğŸ”§ æ­£åœ¨åŠ è½½PDFå¹¶åˆå§‹åŒ–é—®ç­”å¼•æ“..."):
        rag_chain, retriever = init_components(uploaded_file)
    st.success("âœ… PDFåŠ è½½å®Œæˆï¼Œé—®ç­”å¼•æ“å·²å°±ç»ªï¼")
    
    st.divider()
    
    # ======================
    # é—®ç­”åŒºåŸŸ
    # ======================
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")
    question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šæ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆå†…å®¹ï¼Ÿï¼‰")
    
    if st.button("æäº¤é—®é¢˜", type="primary") and question:
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
        with st.expander("ğŸ“Œ æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆLLMå›ç­”ä¾æ®ï¼‰", expanded=True):
            retrieved_docs = retriever.invoke(question)
            for i, doc in enumerate(retrieved_docs):
                st.write(f"### ç›¸å…³æ–‡æ¡£ {i+1}")
                st.write(f"**é¡µç **ï¼š{doc.metadata['page']}")
                content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                st.write(f"**å†…å®¹**ï¼š{content}")
        
        # 2. ç”Ÿæˆå›ç­”ï¼ˆå’Œä½ çš„åŸä»£ç ä¸€è‡´ï¼‰
        with st.spinner("ğŸ¤” æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            answer = rag_chain.invoke(question)
        
        # 3. å±•ç¤ºå›ç­”
        st.subheader("ğŸ“ å›ç­”")
        st.write(answer.strip())
        
        # 4. å±•ç¤ºæ¥æº
        st.subheader("ğŸ“ å›ç­”æ¥æº")
        sources = [f"é¡µç {doc.metadata['page']}" for doc in retrieved_docs]
        st.write(f"æ¥è‡ªï¼š{', '.join(sources)}")

else:
    # æœªä¸Šä¼ PDFæ—¶çš„æç¤º
    st.info("è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ PDFæ–‡ä»¶ï¼Œå¹¶é…ç½®é€šä¹‰åƒé—®APIå¯†é’¥ï¼Œå³å¯å¼€å§‹é—®ç­”ï¼")