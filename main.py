# 1. Python å†…ç½®æ¨¡å—
import os                   # ç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œã€ç›®å½•åˆ›å»º
import re                   # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼ˆæ–‡æœ¬å¤„ç†ã€åˆ†è¯è¿‡æ»¤ï¼‰
import json                 # ç”¨äºæ•°æ®åºåˆ—åŒ–ï¼ˆç¼“å­˜ç›¸å…³ï¼‰
import time                 # ç”¨äºè®¡æ—¶ï¼ˆè¾…åŠ©å¼‚æ­¥æ€§èƒ½ç›‘æ§ï¼‰
import asyncio              # ç”¨äºå¼‚æ­¥å¹¶è¡Œæ£€ç´¢ï¼ˆæå‡æ£€ç´¢æ•ˆç‡ï¼‰
import traceback            # ç”¨äºå¼‚å¸¸å †æ ˆæ‰“å°ï¼ˆé”™è¯¯æ’æŸ¥ï¼‰

# 2. ç¬¬ä¸‰æ–¹å·¥å…·æ¨¡å—
import jieba                # ç”¨äºä¸­æ–‡åˆ†è¯ï¼ˆé€‚é…BM25å…³é”®è¯æ£€ç´¢ï¼‰
from dotenv import load_dotenv  # ç”¨äºåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆLLMå¯†é’¥ç­‰ï¼‰

# 3. ç±»å‹æ³¨è§£ä¸æ•°æ®éªŒè¯æ¨¡å—
from typing import List, Dict, Optional  # ç”¨äºç±»å‹æç¤ºï¼Œæå‡ä»£ç å¯è¯»æ€§
from pydantic import BaseModel, Field    # ç”¨äºæ•°æ®æ¨¡å‹éªŒè¯ï¼ˆæœªç›´æ¥ä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹ï¼‰

# 4. LangChain æ ¸å¿ƒæ¨¡å—
from langchain_core.documents import Document  # å®šä¹‰æ–‡æ¡£æ•°æ®ç»“æ„
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate  # å®šä¹‰æç¤ºè¯æ¨¡æ¿
from langchain_core.output_parsers import StrOutputParser  # LLMè¾“å‡ºæ–‡æœ¬è§£æå™¨
from langchain_core.caches import InMemoryCache  # LLMåŸç”Ÿå†…å­˜ç¼“å­˜
from langchain_core.globals import set_llm_cache  # é…ç½®LLMå…¨å±€ç¼“å­˜
from langchain_core.messages import HumanMessage, AIMessage  # å®šä¹‰å¯¹è¯æ¶ˆæ¯ç±»å‹
from langchain_core.tools import tool  # ç”¨äºå®šä¹‰Agentå¯è°ƒç”¨å·¥å…·

# 5. LangChain æ¨¡å‹ä¸å­˜å‚¨æ¨¡å—
from langchain_openai import ChatOpenAI  # åŠ è½½OpenAIå…¼å®¹æ¨¡å‹ï¼ˆqwen-plusï¼‰
from langchain_huggingface import HuggingFaceEmbeddings  # åŠ è½½HuggingFaceåµŒå…¥æ¨¡å‹
from langchain_chroma import Chroma  # åŠ è½½Chromaå‘é‡æ•°æ®åº“ï¼ˆæ–‡æ¡£å­˜å‚¨ä¸æ£€ç´¢ï¼‰
from langchain_text_splitters import RecursiveCharacterTextSplitter  # æ–‡æœ¬åˆ†å—å™¨ï¼ˆå¤‡ç”¨ï¼Œå®é™…ä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼‰
from langchain_community.cross_encoders import HuggingFaceCrossEncoder  # åŠ è½½äº¤å‰ç¼–ç å™¨ï¼ˆæ–‡æ¡£é‡æ’åºï¼‰

# 6. LangChain Agent æ¨¡å—
from langchain.agents import create_agent  # åˆ›å»ºå·¥å…·è°ƒç”¨Agent

# 7. æ–‡æ¡£åŠ è½½ä¸BM25æ£€ç´¢æ¨¡å—
from llama_index.core import SimpleDirectoryReader  # åŠ è½½PDFæ–‡æ¡£
from rank_bm25 import BM25Okapi  # åŠ è½½BM25å…³é”®è¯æ£€ç´¢å™¨

# 8. è‡ªå®šä¹‰æ¨¡å—
from cache_utils import get_pdf_file_hash, load_pdf_topic_from_cache, save_cache_pdf_topic  # PDFç¼“å­˜å·¥å…·
from layered_memory import LayeredMemoryManager  # åˆ†å±‚è®°å¿†ç®¡ç†å™¨ï¼ˆçŸ­æœŸ+é•¿æœŸè®°å¿†ï¼‰

# ------------------------------ å…¨å±€é…ç½® ------------------------------
load_dotenv()
CONFIG = {
    "qwen_api_key": os.getenv("QWEN_API_KEY"),
    "qwen_base_url": os.getenv("QWEN_BASE_URL"),
    "pdf_path": "./assets/file.pdf",
    "embedding_path": "./models/bge-small-zh-v1.5",
    "rerank_path": "./models/bge-reranker-base",
    "chroma_dir": "./chroma_db",
    "llm_model": "qwen-plus",
    "llm_temperature": 0.1,
    "retrieve_top_k": 2,
    "max_chunk_size": 300, # ä»500ç¼©å°åˆ°300ï¼Œæ›´ç²¾å‡†åˆ‡åˆ†
    "min_chunk_size": 100,
    "chunk_overlap": 30, # é‡å éƒ¨åˆ†ç›¸åº”å‡å°‘
    "short_term_max_rounds": 3 # çŸ­æœŸè®°å¿†æœ€å¤§è½®æ•°
}

# ------------------------------ ç¼“å­˜é…ç½® ------------------------------
# 1. LLMåŸç”Ÿå†…å­˜ç¼“å­˜ï¼ˆæå‡é‡å¤æŸ¥è¯¢æ•ˆç‡ï¼‰
llm_cache = InMemoryCache()
set_llm_cache(llm_cache)

# 2. è‡ªå®šä¹‰ä¸šåŠ¡çº§ç¼“å­˜
qa_cache = {}          # ç¼“å­˜æœ€ç»ˆé—®ç­”ç»“æœï¼škey=æ ‡å‡†åŒ–é—®é¢˜ï¼Œvalue=(å›ç­”, æ¥æº)
relevance_cache = {}   # ç¼“å­˜ç›¸å…³æ€§åˆ¤æ–­ï¼škey=æ ‡å‡†åŒ–é—®é¢˜ï¼Œvalue=(æ˜¯å¦ç›¸å…³, ç†ç”±)

# ------------------------------ æ¨¡å‹åˆå§‹åŒ– ------------------------------
# 1. åµŒå…¥æ¨¡å‹åˆå§‹åŒ–ï¼ˆç”¨äºæ–‡æ¡£å‘é‡åŒ–ï¼Œé€‚é…Chromaå‘é‡åº“ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name=CONFIG["embedding_path"],
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# 2. LLMæ¨¡å‹åˆå§‹åŒ–ï¼ˆqwen-plusï¼Œæ ¸å¿ƒé—®ç­”ä¸æ¨ç†æ¨¡å‹ï¼‰
llm = ChatOpenAI(
    model=CONFIG["llm_model"],
    temperature=CONFIG["llm_temperature"],
    api_key=CONFIG["qwen_api_key"],
    base_url=CONFIG["qwen_base_url"],
    cache=True  # å¼€å¯LLMåŸç”Ÿç¼“å­˜å…œåº•
)

# 3. é‡æ’åºæ¨¡å‹åˆå§‹åŒ–ï¼ˆç”¨äºæ£€ç´¢ç»“æœç²¾æ’ï¼Œæå‡ç›¸å…³æ€§ï¼‰
reranker = HuggingFaceCrossEncoder(
    model_name=CONFIG["rerank_path"],
    model_kwargs={"device": "cpu"}
)

# ------------------------------ æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ------------------------------
# æ ‡å‡†åŒ–é—®é¢˜ï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œç¡®ä¿ç›¸åŒé—®é¢˜å‘½ä¸­ç¼“å­˜ï¼‰
def normalize_question(question):
    return question.strip().lower()

# å¤šæŸ¥è¯¢ç”Ÿæˆæ ¸å¿ƒå‡½æ•°ï¼ˆç”ŸæˆåŒä¹‰æŸ¥è¯¢ï¼Œæå‡æ£€ç´¢å¬å›ç‡ï¼‰
def generate_multi_queries(original_query, llm):
    multi_query_prompt = PromptTemplate(
        template="""ä½ æ˜¯æŸ¥è¯¢ä¼˜åŒ–åŠ©æ‰‹ï¼Œä¸ºåŸå§‹é—®é¢˜ç”Ÿæˆ3ä¸ªåŒä¹‰/ç›¸å…³æŸ¥è¯¢ï¼Œç”¨äºæå‡æ£€ç´¢å¬å›ç‡ã€‚
        è§„åˆ™ï¼š
        1.  ä»…è¿”å›3ä¸ªæŸ¥è¯¢ï¼Œæ¯è¡Œ1ä¸ªï¼Œæ— ç¼–å·ã€æ— é¢å¤–è§£é‡Šï¼›
        2.  è¯­ä¹‰ä¸€è‡´ï¼Œä»…è°ƒæ•´è¡¨è¿°ï¼ˆæ›¿æ¢åŒä¹‰è¯/ä¸“ä¸šæœ¯è¯­ï¼‰ï¼›
        3.  é€‚é…é‡‘èè´¢æŠ¥åœºæ™¯ï¼Œé¿å…å£è¯­åŒ–ï¼›
        4.  ä¸é‡å¤è¡¨è¿°ï¼Œä¸æ·»åŠ å¤šä½™å†…å®¹ã€‚

        åŸå§‹é—®é¢˜ï¼š{original_query}""",
        input_variables=["original_query"]
    )

    # æ„å»ºå¤šæŸ¥è¯¢ç”Ÿæˆé“¾ï¼ˆLangChain 1.0+ Runnable å†™æ³•ï¼‰
    multi_query_chain = multi_query_prompt | llm | StrOutputParser()

    try:
        generated_queries_text = multi_query_chain.invoke({"original_query": original_query})
        generated_queries = [q.strip() for q in generated_queries_text.split("\n") if q.strip()]
        all_queries = list(set(generated_queries + [original_query]))  # å»é‡+åˆå¹¶åŸå§‹æŸ¥è¯¢
        final_queries = [q for q in all_queries if q] or [original_query]  # è¿‡æ»¤ç©ºå€¼
        return final_queries[:4]  # é™åˆ¶æœ€å¤š4ä¸ªæŸ¥è¯¢
    except Exception as e:
        print(f"âš ï¸ å¤šæŸ¥è¯¢ç”Ÿæˆå¼‚å¸¸ï¼š{e}ï¼Œå·²é™çº§ä¸ºåŸå§‹æŸ¥è¯¢")
        return [original_query]

# è¯­ä¹‰åˆ†å—å‡½æ•°ï¼ˆæŒ‰æ®µè½+å¥å­æ‹†åˆ†ï¼Œä¿è¯æ–‡æ¡£è¯­ä¹‰å®Œæ•´æ€§ï¼‰
def semantic_split(docs, max_chunk_len=300, min_chunk_len=100):
    """
    è¯­ä¹‰åˆ†å—å‡½æ•°
    :param docs: åŸå§‹ Document åˆ—è¡¨ï¼ˆä½ çš„ raw_docsï¼‰
    :param max_chunk_len: å•ä¸ªå—æœ€å¤§å­—ç¬¦æ•°
    :param min_chunk_len: å•ä¸ªå—æœ€å°å­—ç¬¦æ•°
    :return: è¯­ä¹‰å®Œæ•´çš„ Document åˆ†å—åˆ—è¡¨
    """
    final_split_docs = []

    for raw_doc in docs:
        raw_text = raw_doc.page_content.strip()
        doc_metadata = raw_doc.metadata  # ä¿ç•™åŸå§‹é¡µç ç­‰å…ƒæ•°æ®

        # æ­¥éª¤1ï¼šä¼˜å…ˆæŒ‰æ®µè½ï¼ˆç©ºè¡Œï¼‰æ‹†åˆ†
        paragraph_chunks = re.split(r'\n\n+', raw_text)

        for para in paragraph_chunks:
            para = para.strip()
            if not para:
                continue

            # æ­¥éª¤2ï¼šæ®µè½å°ºå¯¸åˆé€‚ï¼Œç›´æ¥å°è£…ä¸º Document
            if min_chunk_len <= len(para) <= max_chunk_len:
                para_doc = Document(page_content=para, metadata=doc_metadata)
                final_split_docs.append(para_doc)
                continue

            # æ­¥éª¤3ï¼šæ®µè½è¿‡é•¿ï¼ŒæŒ‰ä¸­æ–‡å¥å­æ‹†åˆ†ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
            if len(para) > max_chunk_len:
                sent_parts = re.split(r'([ã€‚ï¼ï¼Ÿï¼›])', para)
                temp_sent = ""
                for part in sent_parts:
                    temp_sent += part
                    # æ»¡è¶³å°ºå¯¸è¦æ±‚æ—¶ï¼Œå°è£…ä¸º Document
                    if (len(temp_sent) >= min_chunk_len and part in ["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›"]) or len(temp_sent) >= max_chunk_len:
                        if temp_sent.strip():
                            sent_doc = Document(page_content=temp_sent.strip(), metadata=doc_metadata)
                            final_split_docs.append(sent_doc)
                        temp_sent = ""
                # å¤„ç†æœ€åä¸€ä¸ªå‰©ä½™å¥å­
                if temp_sent.strip():
                    last_sent_doc = Document(page_content=temp_sent.strip(), metadata=doc_metadata)
                    final_split_docs.append(last_sent_doc)

    return final_split_docs

# åŠ è½½PDFå¹¶æ‰§è¡Œè¯­ä¹‰åˆ†å—
def load_and_split_pdf():
    # æ ¡éªŒPDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CONFIG["pdf_path"]):
        raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼š{CONFIG['pdf_path']}")
    
    # åŠ è½½PDFæ–‡æ¡£
    llama_docs = SimpleDirectoryReader(input_files=[CONFIG["pdf_path"]]).load_data()
    # è½¬æ¢ä¸ºLangChain Documentæ ¼å¼
    raw_docs = [
        Document(page_content=doc.text.strip(), metadata={"page": doc.metadata.get("page_label", "æœªçŸ¥")})
        for doc in llama_docs
    ]
    print(f"ğŸ“„ PDFåŠ è½½å®Œæˆï¼Œå…±{len(raw_docs)}é¡µ")

    # æ‰§è¡Œè¯­ä¹‰åˆ†å—
    split_docs = semantic_split(
        docs=raw_docs,
        max_chunk_len=CONFIG["max_chunk_size"],
        min_chunk_len=CONFIG["min_chunk_size"]
    )
    print(f"âœ‚ï¸ PDFåˆ†å—å®Œæˆï¼Œå…±{len(split_docs)}ä¸ªç‰‡æ®µ")

    return split_docs

# æå–PDFåŸºç¡€ä¿¡æ¯ï¼ˆæ ¸å¿ƒå…³é”®è¯+ä¸»é¢˜ï¼Œç”¨äºç›¸å…³æ€§åˆ¤æ–­ï¼‰
def get_pdf_basic_info(docs):
    # æå–å‰5ä¸ªæ–‡æ¡£çš„å‰200å­—ä½œä¸ºæ ¸å¿ƒå†…å®¹
    core_content = "\n".join([doc.page_content[:200] for doc in docs[:5]])
    
    # æ„å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", """è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒä¿¡æ¯ï¼š
        1. è¾“å‡º5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
        2. è¾“å‡º1å¥è¯æ€»ç»“æ ¸å¿ƒä¸»é¢˜ï¼ˆä¸è¶…è¿‡80å­—ï¼‰
        è¾“å‡ºæ ¼å¼ï¼šå…ˆå†™å…³é”®è¯ï¼Œæ¢è¡Œåå†™ä¸»é¢˜"""),
        ("human", f"å†…å®¹ï¼š{core_content}")
    ])
    
    # æ„å»ºä¿¡æ¯æå–é“¾
    chain = prompt | llm | StrOutputParser()
    try:
        result = chain.invoke({}).strip()
        lines = result.split("\n")
        
        # è§£æå…³é”®è¯å’Œä¸»é¢˜
        keywords = lines[0].split(",") if len(lines)>=1 else ["æœªçŸ¥"]
        topic = lines[1].strip() if len(lines)>=2 else "æœªçŸ¥PDFæ–‡æ¡£"
        
        # æ¸…æ´—å…³é”®è¯å¹¶è¡¥å…¨5ä¸ª
        keywords = [k.strip() for k in keywords if k.strip()]
        if len(keywords) < 5:
            keywords += ["æœªçŸ¥"] * (5 - len(keywords))
        
        return {"keywords": keywords[:5], "topic": topic}
    except Exception as e:
        print(f"âš ï¸ æå–PDFä¿¡æ¯å¤±è´¥ï¼š{e}")
        return {"keywords": ["æœªçŸ¥"]*5, "topic": "æœªçŸ¥PDFæ–‡æ¡£"}

# æ„å»ºé—®ç­”é“¾ï¼ˆæ ¸å¿ƒï¼šæ•´åˆRAG+Agent+åˆ†å±‚è®°å¿†+å·¥å…·è°ƒç”¨ï¼‰
def build_qa_chain():
    # 1. åŠ è½½PDFå¹¶åˆå§‹åŒ–å‘é‡åº“
    docs = load_and_split_pdf()
    split_docs = docs  # å…³é”®ï¼šè·å–split_docsï¼Œè§£å†³ä½œç”¨åŸŸé—®é¢˜
    # åˆå§‹åŒ–Chromaå‘é‡åº“ï¼Œå­˜å‚¨æ–‡æ¡£åµŒå…¥
    vector_db = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=CONFIG["chroma_dir"]
    )
    # åˆ›å»ºå‘é‡æ£€ç´¢å™¨
    retriever = vector_db.as_retriever(search_kwargs={"k": CONFIG["retrieve_top_k"]})

    # åŠ è½½/æå–PDFåŸºç¡€ä¿¡æ¯ï¼ˆä¼˜å…ˆä»ç¼“å­˜åŠ è½½ï¼‰
    pdf_hash = get_pdf_file_hash(CONFIG["pdf_path"])
    pdf_info = load_pdf_topic_from_cache(pdf_hash)
    if not pdf_info:
        pdf_info = get_pdf_basic_info(docs)
        save_cache_pdf_topic(pdf_hash, pdf_info)

    # ========== BM25 å…³é”®è¯æ£€ç´¢å™¨åˆå§‹åŒ–ï¼ˆè§£å†³å‘é‡æ£€ç´¢å¯¹å…³é”®è¯ä¸æ•æ„Ÿé—®é¢˜ï¼‰ ==========
    # 1. æå–æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µçš„æ–‡æœ¬å’Œå…ƒæ•°æ®
    doc_texts = [doc.page_content for doc in split_docs]
    doc_metadata = [doc.metadata for doc in split_docs]

    # 2. ä¸­æ–‡åˆ†è¯å‡½æ•°ï¼ˆé€‚é…BM25æ£€ç´¢ï¼‰
    def chinese_tokenizer(text):
        # è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™æœ‰æ•ˆæ–‡æœ¬
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\@\.]', ' ', text)
        # é€šç”¨åœç”¨è¯è¡¨
        stop_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "è¿™ä¸ª", "é‚£ä¸ª"}
        # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
        tokens = [
            token.strip()
            for token in jieba.cut(text)
            if token.strip() not in stop_words
        ]
        return tokens

    # 3. å¯¹æ–‡æ¡£é›†è¿›è¡Œåˆ†è¯ï¼Œåˆå§‹åŒ–BM25
    tokenized_corpus = [chinese_tokenizer(text) for text in doc_texts]
    bm25 = BM25Okapi(tokenized_corpus)

    # 4. BM25 å…³é”®è¯æ£€ç´¢å‡½æ•°ï¼ˆå¸¦æ ¸å¿ƒè¯åŠ æƒï¼Œæå‡é‡‘èæœ¯è¯­åŒ¹é…åº¦ï¼‰
    def bm25_retrieve(query, top_k=2):
        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenized_query = chinese_tokenizer(query)
        # é‡‘èè´¢æŠ¥æ ¸å¿ƒè¯ï¼ˆåŠ æƒæå‡åŒ¹é…ä¼˜å…ˆçº§ï¼‰
        core_words = {"è¥æ”¶", "å¢é€Ÿ", "åŒæ¯”", "å‡€åˆ©æ¶¦", "åˆ©æ¶¦", "æ•°æ®"}
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„å¾—åˆ†ï¼ˆæ ¸å¿ƒè¯æƒé‡Ã—2ï¼Œæ™®é€šè¯æƒé‡Ã—1ï¼‰
        scores = []
        for doc_tokens in tokenized_corpus:
            score = 0
            for token in tokenized_query:
                if token in core_words:
                    score += bm25.idf.get(token, 0) * (doc_tokens.count(token) / len(doc_tokens)) * 2
                else:
                    score += bm25.idf.get(token, 0) * (doc_tokens.count(token) / len(doc_tokens)) * 1
            scores.append(score)
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå–Top-Kæ–‡æ¡£
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        top_docs = [
            Document(page_content=doc_texts[idx], metadata=doc_metadata[idx])
            for idx in top_indices
        ]
        return top_docs

    # ========== å¼‚æ­¥å¹¶è¡Œæ£€ç´¢è¾…åŠ©å‡½æ•°ï¼ˆæŠµæ¶ˆå¤šæŸ¥è¯¢å¸¦æ¥çš„è€—æ—¶å¢åŠ ï¼‰ ==========
    # å•ä¸ªæŸ¥è¯¢çš„å¼‚æ­¥æ£€ç´¢ï¼šå¹¶è¡Œæ‰§è¡Œ BM25 å’Œ å‘é‡æ£€ç´¢
    async def async_retrieve_single_query(single_query, bm25_retrieve, retriever, top_k):
        # ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥å‡½æ•°ï¼Œå®ç°å¹¶è¡Œæ‰§è¡Œ
        bm25_docs_task = asyncio.to_thread(bm25_retrieve, single_query, top_k)
        vector_docs_task = asyncio.to_thread(retriever.invoke, single_query)
        # å¹¶è¡Œç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆï¼Œè·å–ç»“æœ
        bm25_docs, vector_docs = await asyncio.gather(bm25_docs_task, vector_docs_task)
        return bm25_docs + vector_docs

    # å¤šæŸ¥è¯¢å¼‚æ­¥æ£€ç´¢ä¸»å‡½æ•°ï¼šç”Ÿæˆå¤šæŸ¥è¯¢â†’å¹¶è¡Œæ£€ç´¢â†’åˆå¹¶å»é‡
    async def async_multi_query_retrieve(original_query, llm, bm25_retrieve, retriever, top_k=2):
        # æ­¥éª¤1ï¼šç”Ÿæˆå¤šæŸ¥è¯¢
        multi_queries = generate_multi_queries(original_query, llm)
        print(f"\nğŸ” ç”Ÿæˆå¤šæŸ¥è¯¢åˆ—è¡¨ï¼š{multi_queries}ï¼ˆå…±{len(multi_queries)}ä¸ªï¼‰")

        # æ­¥éª¤2ï¼šåˆ›å»ºæ‰€æœ‰æŸ¥è¯¢çš„å¼‚æ­¥æ£€ç´¢ä»»åŠ¡
        tasks = []
        for single_query in multi_queries:
            task = asyncio.create_task(async_retrieve_single_query(single_query, bm25_retrieve, retriever, top_k))
            tasks.append(task)

        # æ­¥éª¤3ï¼šå¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        all_query_results = await asyncio.gather(*tasks)

        # æ­¥éª¤4ï¼šåˆå¹¶æ‰€æœ‰æ£€ç´¢ç»“æœå¹¶å»é‡
        all_retrieved_docs = []
        doc_content_map = {}  # æŒ‰å†…å®¹å»é‡ï¼Œé¿å…é‡å¤æ–‡æ¡£
        for query_docs in all_query_results:
            for doc in query_docs:
                doc_content = doc.page_content.strip()
                if doc_content not in doc_content_map:
                    doc_content_map[doc_content] = doc
                    all_retrieved_docs.append(doc)

        return all_retrieved_docs

    # åŒæ­¥åŒ…è£…å‡½æ•°ï¼ˆé€‚é…ç°æœ‰åŒæ­¥ä»£ç æ¶æ„ï¼Œæ— éœ€é‡æ„å…¶ä»–é€»è¾‘ï¼‰
    def multi_query_parallel_retrieve(original_query, llm, bm25_retrieve, retriever, top_k=2):
        return asyncio.run(async_multi_query_retrieve(original_query, llm, bm25_retrieve, retriever, top_k))

    # æ–‡æ¡£é‡æ’åºå‡½æ•°ï¼ˆå¯¹æ£€ç´¢ç»“æœç²¾æ’ï¼Œæå‡ç›¸å…³æ€§æ’åºå‡†ç¡®æ€§ï¼‰
    def rerank_docs(query, retrieved_docs):
        """
        å¯¹æ··åˆæ£€ç´¢åçš„æ–‡æ¡£è¿›è¡Œç²¾ç»†è¯­ä¹‰æ’åº
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param retrieved_docs: æ··åˆæ£€ç´¢å¾—åˆ°çš„ Document åˆ—è¡¨
        :return: é‡æ’åºåçš„ Document åˆ—è¡¨ï¼ˆå¼‚å¸¸æ—¶è¿”å›åŸåˆ—è¡¨ï¼‰
        """
        # å¼ºå…œåº•ï¼šæ¨¡å‹æœªåˆå§‹åŒ–/æ— æŸ¥è¯¢/æ— å€™é€‰æ–‡æ¡£ï¼Œç›´æ¥è¿”å›åŸåˆ—è¡¨ï¼Œä¸å½±å“åŸæœ‰åŠŸèƒ½
        if not reranker or not query.strip() or not retrieved_docs:
            return retrieved_docs

        try:
            # æ„é€  (query, doc_content) é…å¯¹åˆ—è¡¨ï¼Œé€‚é…äº¤å‰ç¼–ç å™¨è¾“å…¥
            query_doc_pairs = [(query, doc.page_content) for doc in retrieved_docs]
            # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
            scores = reranker.score(query_doc_pairs)

            # æ‰“å°å¾—åˆ†ï¼Œæ–¹ä¾¿è°ƒè¯•
            print(f"\n===== Rerank ç›¸å…³æ€§å¾—åˆ† =====")
            for idx, score in enumerate(scores):
                print(f"æ–‡æ¡£{idx+1} ç›¸å…³æ€§å¾—åˆ†ï¼š{score}")
                
            # ç»‘å®šæ–‡æ¡£ä¸å¾—åˆ†ï¼ŒæŒ‰å¾—åˆ†é™åºæ’åº
            doc_score_pairs = list(zip(retrieved_docs, scores))
            doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
            # è¿”å›æ’åºåçš„æ–‡æ¡£
            return [pair[0] for pair in doc_score_pairs]
        except Exception as e:
            # å¼‚å¸¸æ—¶å…œåº•è¿”å›åŸå§‹æ–‡æ¡£ï¼Œé¿å…æ£€ç´¢æµç¨‹ä¸­æ–­
            print(f"âš ï¸ Rerank æ‰§è¡Œå¼‚å¸¸ï¼Œè¿”å›åŸå§‹æ£€ç´¢ç»“æœï¼š{e}")
            return retrieved_docs

    # ========== æ··åˆæ£€ç´¢å‡½æ•°ï¼ˆBM25+Chroma+å¤šæŸ¥è¯¢å¹¶è¡Œ+Rerankï¼Œæ ¸å¿ƒæ£€ç´¢é€»è¾‘ï¼‰ ==========
    def hybrid_retrieve(query: str, top_k: int = 2) -> list[Document]:
        """
        æ··åˆæ£€ç´¢å‡½æ•°ï¼ˆBM25å…³é”®è¯æ£€ç´¢ + Chromaå‘é‡æ£€ç´¢ï¼‰
        æ ¸å¿ƒåŠŸèƒ½ï¼šèåˆå…³é”®è¯åŒ¹é…çš„ç²¾å‡†æ€§å’Œè¯­ä¹‰åŒ¹é…çš„æ³›åŒ–æ€§ï¼Œæå‡æ£€ç´¢å¬å›ç‡å’Œç²¾å‡†åº¦
        :param query: ç”¨æˆ·æŸ¥è¯¢è¯­å¥
        :param top_k: å•æ£€ç´¢ç­–ç•¥è¿”å›çš„é¡¶éƒ¨ç‰‡æ®µæ•°é‡
        :return: åˆå¹¶å»é‡åçš„é«˜è´¨é‡æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        # å‚æ•°åˆæ³•æ€§æ ¡éªŒ
        if not isinstance(query, str) or len(query.strip()) == 0:
            return []
        if not isinstance(top_k, int) or top_k < 1 or top_k > 10:
            top_k = 2

        # 2. å¤šæŸ¥è¯¢å¼‚æ­¥å¹¶è¡Œæ£€ç´¢ï¼ˆæ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šæå‡å¬å›ç‡çš„åŒæ—¶ä¿è¯æ€§èƒ½ï¼‰
        all_retrieved_docs = multi_query_parallel_retrieve(
            original_query=query,
            llm=llm,
            bm25_retrieve=bm25_retrieve,
            retriever=retriever,
            top_k=top_k
        )

        # 3. ç»“æœå»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹å»é‡ï¼Œé¿å…é‡å¤ç‰‡æ®µå¹²æ‰°åç»­LLMæ¨ç†ï¼‰
        doc_content_unique_map = {}  # key: æ–‡æ¡£å†…å®¹ï¼ˆå»é‡æ ‡è¯†ï¼‰, value: Documentå¯¹è±¡
        for doc in all_retrieved_docs:
            doc_content = doc.page_content.strip()
            if doc_content not in doc_content_unique_map:
                doc_content_unique_map[doc_content] = doc

        # 4. ç»“æœè£å‰ªï¼ˆæ§åˆ¶è¿”å›æ•°é‡ï¼Œé¿å…è¿‡å¤šç‰‡æ®µå¢åŠ LLMæ¨ç†æˆæœ¬ï¼‰
        final_retrieved_docs = list(doc_content_unique_map.values())[:top_k * 2]  # å–2å€top_kï¼Œå…¼é¡¾å¬å›ç‡å’Œæ€§èƒ½
        # 5. æ–‡æ¡£é‡æ’åºï¼ˆæå‡æ£€ç´¢ç»“æœç›¸å…³æ€§ï¼‰
        final_retrieved_docs = rerank_docs(query, final_retrieved_docs)

        return final_retrieved_docs

    # 3. åˆå§‹åŒ–åˆ†å±‚è®°å¿†ï¼ˆçŸ­æœŸè®°å¿†+é•¿æœŸè®°å¿†ï¼Œæå‡å¯¹è¯è¿è´¯æ€§ï¼‰
    memory_manager = LayeredMemoryManager(
        llm=llm,
        short_term_max_rounds=CONFIG["short_term_max_rounds"]
    )

    # 5. æ„å»ºé™çº§ç”¨å›ç­”é“¾ï¼ˆAgentè°ƒç”¨å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸºç¡€RAGå…œåº•ï¼‰
    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œä»…åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š
        1. ä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸ç¼–é€ ä¿¡æ¯
        2. ç»“åˆå†å²å¯¹è¯çš„æ ¸å¿ƒä¿¡æ¯å’Œæœ€æ–°ä¸Šä¸‹æ–‡ç†è§£é—®é¢˜
        3. å›ç­”ç®€æ´æ˜äº†ï¼Œç›´å‡»è¦ç‚¹"""),
        ("human", """å†å²å¯¹è¯æ ¸å¿ƒä¿¡æ¯ï¼š{memory}
        æ–‡æ¡£å†…å®¹ï¼š{context}
        é—®é¢˜ï¼š{question}""")
    ])

    # æ„å»ºåŸºç¡€å›ç­”é“¾
    answer_chain = answer_prompt | llm | StrOutputParser()

    # ========== Agent å·¥å…·å®šä¹‰ï¼ˆ3ä¸ªæ ¸å¿ƒå·¥å…·ï¼Œè¦†ç›–PDFæŸ¥è¯¢æ ¸å¿ƒåœºæ™¯ï¼‰ ==========
    @tool
    def pdf_meta_query_tool() -> str:
        """
        ä¸“é—¨ç”¨äºæŸ¥è¯¢å½“å‰PDFçš„æ ¸å¿ƒå…ƒä¿¡æ¯ï¼ˆä¸»é¢˜+5ä¸ªå…³é”®è¯ï¼‰ã€‚
        å½“ç”¨æˆ·æé—®ç±»ä¼¼ã€Œè¿™ä»½PDFè®²äº†ä»€ä¹ˆï¼Ÿã€ã€ŒPDFå…³é”®è¯æœ‰å“ªäº›ï¼Ÿã€ã€Œæ–‡æ¡£ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿã€æ—¶è°ƒç”¨ã€‚
        è¾“å‡ºè¿”å›PDFä¸»é¢˜å’Œå…³é”®è¯çš„åŸå§‹ä¿¡æ¯ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–è¯æœ¯ã€å¯’æš„ã€å¼•å¯¼æé—®ç­‰æ— å…³å†…å®¹ã€‚
        """
        meta_info = f"""
        PDFæ ¸å¿ƒå…ƒä¿¡æ¯
        - æ ¸å¿ƒä¸»é¢˜ï¼š{pdf_info['topic']}
        - æ ¸å¿ƒå…³é”®è¯ï¼š{', '.join(pdf_info['keywords'])}
        """
        return meta_info.strip()

    @tool
    def pdf_vector_retrieve_tool(query: str, top_k: int = 2) -> str:
        """
        ä¸“é—¨ç”¨äºæ£€ç´¢PDFä¸­ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„ç²¾å‡†æ–‡æœ¬ç‰‡æ®µï¼Œç”¨äºå›ç­”å…·ä½“æ¦‚å¿µã€æ•°æ®ã€æ–¹æ³•ç­‰éä¸»é¢˜ç±»é—®é¢˜ã€‚
        è§¦å‘æ¡ä»¶ï¼šå½“ç”¨æˆ·æé—®ä¸æ¶‰åŠPDFä¸»é¢˜/å…³é”®è¯ï¼Œä¹Ÿä¸æ¶‰åŠé¡µç æ—¶è°ƒç”¨ã€‚
        å‚æ•°è¯´æ˜ï¼š
        - queryï¼šå¿…å¡«é¡¹ï¼Œä¼ å…¥ç”¨æˆ·å®Œæ•´æŸ¥è¯¢é—®é¢˜ã€‚
        - top_kï¼šå¯é€‰é¡¹ï¼Œæ£€ç´¢è¿”å›çš„æ–‡æœ¬ç‰‡æ®µæ•°é‡ï¼Œé»˜è®¤2ã€‚
        """
        try:
            # ç®€å•å‚æ•°æ ¡éªŒ
            if not query.strip():
                return "é”™è¯¯ï¼šæŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©ºï¼"
            top_k = max(1, min(top_k, 5))  # é™åˆ¶top_kèŒƒå›´ï¼Œé¿å…æ— æ•ˆæ£€ç´¢
            
            # æ”¹ç”¨æ··åˆæ£€ç´¢ï¼ˆæ ¸å¿ƒï¼šæå‡æ£€ç´¢æ•ˆæœï¼‰
            # åŸä»£ç ï¼šretrieved_docs = retriever.invoke(query)
            retrieved_docs = hybrid_retrieve(query, top_k=top_k)
            
            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼Œé™„å¸¦é¡µç ä¿¡æ¯
            context_str = ""
            for idx, doc in enumerate(retrieved_docs, 1):
                page_num = doc.metadata.get("page", "æœªçŸ¥")
                context_str += f"ã€æ£€ç´¢ç‰‡æ®µ{idx}ï¼ˆé¡µç ï¼š{page_num}ï¼‰ã€‘\n{doc.page_content}\n\n"
            
            return f"æ£€ç´¢æˆåŠŸï¼ˆå…±æ‰¾åˆ°{len(retrieved_docs)}ä¸ªç›¸å…³ç‰‡æ®µï¼‰ï¼š\n{context_str.strip()}"
        except Exception as e:
            return f"å·¥å…·è°ƒç”¨å¤±è´¥ï¼š{str(e)}"

    @tool
    def pdf_page_search_tool(page_num: str) -> str:
        """
        ä¸“é—¨ç”¨äºæŒ‰é¡µç æŸ¥è¯¢PDFå¯¹åº”å†…å®¹ï¼Œæ”¯æŒå•ä¸ªé¡µç ï¼ˆå¦‚ã€Œ3ã€ï¼‰å’Œé¡µç èŒƒå›´ï¼ˆå¦‚ã€Œ5-8ã€ï¼‰ï¼Œä¹Ÿæ”¯æŒå¸¦ã€Œç¬¬ã€ã€Œé¡µã€çš„è¡¨è¿°ï¼ˆå¦‚ã€Œç¬¬4é¡µã€ï¼‰ã€‚
        å‚æ•°è¯´æ˜ï¼š
        - page_numï¼šå¿…å¡«ï¼Œè¦æŸ¥è¯¢çš„é¡µç ï¼ˆå¦‚ã€Œ3ã€ã€Œ5-8ã€ã€Œç¬¬6é¡µã€ï¼‰
        """
        try:
            # æ¸…ç†é¡µç å‚æ•°ï¼Œä»…ä¿ç•™æ•°å­—å’Œæ¨ªæ 
            clean_page = ''.join([c for c in page_num if c.isdigit() or c == '-'])
            if not clean_page:
                return "é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆçš„é¡µç ï¼ˆå¦‚ã€Œ3ã€ã€Œ5-8ã€ï¼‰ï¼"
            
            # é‡æ–°åŠ è½½PDFæ–‡æ¡£
            all_docs = load_and_split_pdf()
            page_content = []
            
            # å¤„ç†é¡µç èŒƒå›´
            if '-' in clean_page:
                start_page, end_page = clean_page.split('-')
                start_page = int(start_page) if start_page.isdigit() else 1
                end_page = int(end_page) if end_page.isdigit() else start_page
            else:
                start_page = end_page = int(clean_page) if clean_page.isdigit() else 1
            
            # ç­›é€‰å¯¹åº”é¡µç çš„æ–‡æ¡£å†…å®¹
            for doc in all_docs:
                doc_page = doc.metadata.get("page", "æœªçŸ¥")
                try:
                    doc_page_int = int(doc_page)
                    if start_page <= doc_page_int <= end_page:
                        page_content.append(f"ã€é¡µç {doc_page}ã€‘\n{doc.page_content}")
                except:
                    continue
            
            # æ— ç»“æœæ—¶æç¤º
            if not page_content:
                return f"æœªæ‰¾åˆ°é¡µç {page_num}å¯¹åº”çš„PDFå†…å®¹ï¼Œè¯·ç¡®è®¤é¡µç æœ‰æ•ˆã€‚"
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            return f"é¡µç {page_num}å¯¹åº”å†…å®¹ï¼š\n{chr(10).join(page_content)}"
        except Exception as e:
            return f"å·¥å…·è°ƒç”¨å¤±è´¥ï¼š{str(e)}"

    # å·¥å…·åˆ—è¡¨ï¼ˆä¾›Agentè°ƒç”¨ï¼‰
    tools = [pdf_meta_query_tool, pdf_vector_retrieve_tool, pdf_page_search_tool]

    # ========== Agent é…ç½®ï¼ˆæ€è€ƒé“¾+ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰Agentè¡Œä¸ºé€»è¾‘ï¼‰ ==========
    # Agent ç³»ç»Ÿæç¤ºè¯ï¼ˆæ˜ç¡®æ€è€ƒæµç¨‹ã€å·¥å…·ä½¿ç”¨è§„åˆ™ã€å›ç­”è¦æ±‚ï¼‰
    system_prompt_str = """
    ä½ æ˜¯ä¸€ä¸ªå…·å¤‡æ·±åº¦æ¨ç†èƒ½åŠ›çš„PDFä¸“ä¸šé—®ç­”åŠ©æ‰‹ï¼Œä»…å¯ä½¿ç”¨æä¾›çš„3ä¸ªå·¥å…·è§£å†³é—®é¢˜ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹æµç¨‹ï¼š
    1.  ç¬¬ä¸€æ­¥ï¼šå…ˆæ€è€ƒï¼ˆå¿…é¡»æ‰§è¡Œï¼Œç¦æ­¢è·³è¿‡ï¼‰
        è¯·ä½ å…ˆåˆ†æç”¨æˆ·é—®é¢˜ï¼Œæ‹†è§£è§£å†³é—®é¢˜çš„æ­¥éª¤ï¼Œå¹¶åˆ¤æ–­æ¯ä¸€æ­¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼š
        - æ€è€ƒè¦ç‚¹1ï¼šè¿™ä¸ªé—®é¢˜éœ€è¦æ‹†è§£æˆå‡ ä¸ªå°æ­¥éª¤ï¼Ÿæ¯ä¸ªæ­¥éª¤çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
        - æ€è€ƒè¦ç‚¹2ï¼šæ¯ä¸ªæ­¥éª¤æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Ÿå¦‚æœéœ€è¦ï¼Œåº”è¯¥é€‰æ‹©å“ªä¸ªå·¥å…·ï¼Ÿä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå·¥å…·ï¼Ÿ
        - æ€è€ƒè¦ç‚¹3ï¼šè°ƒç”¨å·¥å…·éœ€è¦çš„å‚æ•°æ˜¯å¦å®Œæ•´ï¼Ÿï¼ˆå¦‚ pdf_vector_retrieve_tool éœ€è¦ queryï¼Œpdf_page_search_tool éœ€è¦ page_numï¼‰
        - æ€è€ƒè¦ç‚¹4ï¼šå¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œæ˜¯å¦éœ€è¦è°ƒæ•´å‚æ•°é‡è¯•ï¼Ÿï¼ˆå¦‚æ£€ç´¢ä¸åˆ°å†…å®¹æ—¶ï¼Œæ˜¯å¦éœ€è¦æ‰©å±•å…³é”®è¯ï¼‰
    2.  ç¬¬äºŒæ­¥ï¼šå·¥å…·è°ƒç”¨ï¼ˆä¸¥æ ¼æŒ‰æ€è€ƒç»“æœæ‰§è¡Œï¼‰
        å·¥å…·åŒ¹é…è§„åˆ™ï¼ˆä»…å‚è€ƒï¼Œéœ€ç»“åˆæ€è€ƒçµæ´»è°ƒæ•´ï¼‰ï¼š
        - å½“éœ€è¦æŸ¥è¯¢PDFçš„æ ¸å¿ƒä¸»é¢˜æˆ–5ä¸ªæ ¸å¿ƒå…³é”®è¯æ—¶ï¼Œè°ƒç”¨ pdf_meta_query_toolï¼ˆæ— å‚æ•°ï¼‰
        - å½“éœ€è¦æ£€ç´¢PDFä¸­ä¸å…·ä½“é—®é¢˜ï¼ˆå¦‚ä¸šç»©ã€æ•°æ®ã€æ¦‚å¿µå®šä¹‰ï¼‰ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µæ—¶ï¼Œè°ƒç”¨ pdf_vector_retrieve_toolï¼ˆå¿…å¡«å‚æ•°ï¼šquery=ç”¨æˆ·å®Œæ•´é—®é¢˜ï¼›å¯é€‰å‚æ•°ï¼štop_k=2ï¼‰
        - å½“éœ€è¦æŒ‰é¡µç /é¡µç èŒƒå›´æŸ¥è¯¢PDFå…·ä½“å†…å®¹æ—¶ï¼Œè°ƒç”¨ pdf_page_search_toolï¼ˆå¿…å¡«å‚æ•°ï¼špage_num=ç”¨æˆ·æŒ‡å®šçš„é¡µç /é¡µç èŒƒå›´ï¼‰
        - å½“é—®é¢˜æ˜¯é—²èŠã€æ— å…³çŸ¥è¯†ï¼ˆå¦‚å¤©æ°”ã€è®¡ç®—ï¼‰æ—¶ï¼Œæ— éœ€è°ƒç”¨å·¥å…·ï¼Œç›´æ¥è¿”å›æ˜ç¡®å›ç­”
    3.  ç¬¬ä¸‰æ­¥ï¼šç»“æœå¤„ç†ï¼ˆå¯é€‰ï¼šäºŒæ¬¡æ€è€ƒï¼‰
        - è‹¥è·å–å·¥å…·è¿”å›ç»“æœåï¼Œå·²èƒ½å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œç›´æ¥æ±‡æ€»æ•´ç†ç»“æœï¼Œä¿æŒç®€æ´æ˜äº†ï¼›
        - è‹¥å·¥å…·è¿”å›ç»“æœä¸å®Œæ•´ï¼Œæ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œéœ€è¦å†æ¬¡æ€è€ƒï¼šæ˜¯å¦éœ€è¦è¡¥å……è°ƒç”¨å…¶ä»–å·¥å…·ï¼Ÿæˆ–è°ƒæ•´å‚æ•°é‡æ–°è°ƒç”¨åŒä¸€å·¥å…·ï¼Ÿ
    4.  å›ç­”è¦æ±‚ï¼š
        - ä¸¥æ ¼åŸºäºå·¥å…·è¿”å›ç»“æœç”Ÿæˆå›ç­”ï¼Œç»ä¸ç¼–é€ ä»»ä½•ä¿¡æ¯ï¼›
        - å¦‚æœå·¥å…·è¿”å›çš„æ£€ç´¢ç‰‡æ®µä¸­**æ²¡æœ‰ç›¸å…³å†…å®¹**ï¼Œç›´æ¥å›å¤ã€Œæœªæ£€ç´¢åˆ°ä¸è¥æ”¶å¢é€Ÿç›¸å…³çš„æœ‰æ•ˆä¿¡æ¯ï¼Œè¯·è°ƒæ•´å…³é”®è¯é‡è¯•ã€ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•çŒœæµ‹æ€§æ•°æ®ï¼›
        - ä¿ç•™å…³é”®æº¯æºä¿¡æ¯ï¼ˆå¦‚é¡µç ã€æ£€ç´¢ç‰‡æ®µç¼–å·ï¼‰ï¼Œæå‡å›ç­”å¯ä¿¡åº¦ï¼›
        - å·¥å…·è°ƒç”¨å¤±è´¥æ—¶ï¼Œå¦‚å®åé¦ˆå¤±è´¥åŸå› ï¼ˆå¦‚â€œæœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹â€â€œé¡µç æ— æ•ˆâ€ï¼‰ï¼Œä¸å¼ºè¡Œå›ç­”ï¼›
    """

    # åˆ›å»ºAgentï¼ˆLangChain 1.0+ è§„èŒƒï¼Œæ— AgentExecutorï¼‰
    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt=system_prompt_str,
    )

    # ========== ç›¸å…³æ€§åˆ¤æ–­å‡½æ•°ï¼ˆè¿‡æ»¤æ— å…³é—®é¢˜ï¼Œæå‡é—®ç­”æ•ˆç‡ï¼‰ ==========
    def is_question_relevant(question):
        q_normalized = normalize_question(question)
        
        # 1. ä¼˜å…ˆæŸ¥è‡ªå®šä¹‰ç¼“å­˜ï¼Œæå‡æ•ˆç‡
        if q_normalized in relevance_cache:
            print(f"ğŸ” å‘½ä¸­ç›¸å…³æ€§ç¼“å­˜ï¼š{question}")
            return relevance_cache[q_normalized]
        
        # 2. æœªå‘½ä¸­ç¼“å­˜æ—¶ï¼Œè°ƒç”¨LLMåˆ¤æ–­ç›¸å…³æ€§
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""è¯·åˆ¤æ–­é—®é¢˜æ˜¯å¦å’Œä»¥ä¸‹ä¸»é¢˜ç›¸å…³ï¼š
            æ ¸å¿ƒä¸»é¢˜ï¼š{pdf_info['topic']}
            æ ¸å¿ƒå…³é”®è¯ï¼š{','.join(pdf_info['keywords'])}
            ç›¸å…³ï¼šé—®é¢˜å›´ç»•PDFå†…å®¹ã€ä¸»é¢˜ã€å…³é”®è¯å±•å¼€
            éç›¸å…³ï¼šå¤©æ°”ã€é—²èŠã€æ— å…³çŸ¥è¯†ã€çº¯è®¡ç®—ç­‰
            è¾“å‡ºæ ¼å¼ï¼šå…ˆå†™ã€ç›¸å…³ã€‘æˆ–ã€éç›¸å…³ã€‘ï¼Œç©ºæ ¼åå†™ç†ç”±"""),
            ("human", f"é—®é¢˜ï¼š{question}")
        ])
        
        chain = prompt | llm | StrOutputParser()
        try:
            print(f"ğŸ“¡ è°ƒç”¨LLMåˆ¤æ–­ç›¸å…³æ€§ï¼š{question}")
            result = chain.invoke({}).strip()
            
            # è§£æåˆ¤æ–­ç»“æœ
            if result.startswith("ã€ç›¸å…³ã€‘"):
                is_rel = True
                reason = result.replace("ã€ç›¸å…³ã€‘", "").strip()
            else:
                is_rel = False
                reason = result.replace("ã€éç›¸å…³ã€‘", "").strip() if "ã€éç›¸å…³ã€‘" in result else "é—®é¢˜ä¸PDFå†…å®¹æ— å…³"
            
            # å­˜å…¥ç¼“å­˜ï¼Œåç»­å¤ç”¨
            relevance_cache[q_normalized] = (is_rel, reason)
            return is_rel, reason
        except Exception as e:
            print(f"âš ï¸ åˆ¤æ–­ç›¸å…³æ€§å¤±è´¥ï¼š{e}")
            return False, "æ— æ³•åˆ¤æ–­ç›¸å…³æ€§"

    # ========== ä¸»é—®ç­”å‡½æ•°ï¼ˆæ•´åˆæ‰€æœ‰é€»è¾‘ï¼Œå¯¹å¤–æä¾›é—®ç­”æ¥å£ï¼‰ ==========
    def qa_function(question):
        # ç©ºå€¼æ ¡éªŒ
        question_clean = question.strip()
        if not question_clean:
            return "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼", []
        
        # æ ‡å‡†åŒ–é—®é¢˜
        q_normalized = normalize_question(question_clean)
        
        # 1. ä¼˜å…ˆæŸ¥æœ€ç»ˆé—®ç­”ç»“æœç¼“å­˜ï¼Œæå‡æ•ˆç‡
        if q_normalized in qa_cache:
            print(f"ğŸ” å‘½ä¸­é—®ç­”ç¼“å­˜ï¼š{question_clean}")
            return qa_cache[q_normalized]
        
        # 2. åˆ¤æ–­é—®é¢˜ç›¸å…³æ€§ï¼Œè¿‡æ»¤æ— å…³é—®é¢˜
        is_relevant, reason = is_question_relevant(question_clean)
        if not is_relevant:
            result = (f"âŒ æŠ±æ­‰ï¼Œæˆ‘ä»…èƒ½å›ç­”ä¸PDFç›¸å…³çš„é—®é¢˜å“¦ï½ï¼ˆåŸå› ï¼š{reason}ï¼‰", [])
            qa_cache[q_normalized] = result
            return result
        
        # 3. è·å–åˆ†å±‚è®°å¿†ï¼ˆçŸ­æœŸ+é•¿æœŸï¼‰ï¼Œæå‡å¯¹è¯è¿è´¯æ€§
        combined_memory = memory_manager.get_combined_memory()

        # 4. è°ƒç”¨Agentå·¥å…·é“¾ç”Ÿæˆå›ç­”
        try:
            # æ„é€ Agentè¾“å…¥
            inputs = {
                "messages": [
                    {"role": "user", "content": f"å†å²å¯¹è¯è®°å¿†ï¼š{combined_memory}\nç”¨æˆ·é—®é¢˜ï¼š{question_clean}"}
                ]
            }
            answer = ""
            # éå†Agent streamè¾“å‡ºï¼Œæå–æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
            for chunk in agent.stream(inputs, stream_mode="updates"):
                if "model" in chunk and len(chunk["model"]["messages"]) > 0:
                    latest_msg = chunk["model"]["messages"][-1]
                    # æ‰“å°å·¥å…·è°ƒç”¨å†³ç­–ï¼ˆæ–¹ä¾¿è°ƒè¯•å’Œå±•ç¤ºAgentæ€è€ƒè¿‡ç¨‹ï¼‰
                    if hasattr(latest_msg, "tool_calls") and latest_msg.tool_calls:
                        for tool_call in latest_msg.tool_calls:
                            tool_name = tool_call["name"]
                            tool_args = tool_call["args"]
                            tool_call_id = tool_call["id"]
                            print("=" * 30 + " Agent æ€è€ƒè¿‡ç¨‹ " + "=" * 30)
                            print(f"ğŸ¤” æ¨ç†ç›®æ ‡ï¼šè§£å†³ç”¨æˆ·é—®é¢˜ã€Œ{question_clean}ã€")
                            print(f"âœ… å†³ç­–ç»“æœï¼šè°ƒç”¨å·¥å…·ã€Œ{tool_name}ã€")
                            print(f"ğŸ“‹ å·¥å…·å‚æ•°ï¼š{tool_args}")
                            print(f"ğŸ†” è°ƒç”¨IDï¼š{tool_call_id}")
                            print(f"ğŸ“ æ¨ç†åŸå› ï¼šè¯¥å·¥å…·æ˜¯è·å–ã€Œ{question_clean}ã€ç›¸å…³ä¿¡æ¯çš„æœ€ä¼˜é€‰æ‹©ï¼Œå¯ç›´æ¥æ»¡è¶³æŸ¥è¯¢éœ€æ±‚")
                            print("=" * 68)
                    
                    # æå–æœ‰æ•ˆå›ç­”å†…å®¹
                    if latest_msg.content.strip():
                        answer = latest_msg.content
                        # æ‰“å°Agentè‡ªç„¶è¯­è¨€æ€è€ƒï¼ˆå¯é€‰ï¼Œæå‡å¯è§£é‡Šæ€§ï¼‰
                        if "æˆ‘éœ€è¦å…ˆ" in latest_msg.content or "ç¬¬ä¸€æ­¥" in latest_msg.content:
                            print("=" * 30 + " Agent è¯­ä¹‰æ€è€ƒ " + "=" * 30)
                            print(f"ğŸ’¡ è‡ªç„¶è¯­è¨€æ€è€ƒï¼š{latest_msg.content}")
                            print("=" * 68)
            
            # å…œåº•ï¼šAgentæœªè¿”å›æœ‰æ•ˆå›ç­”æ—¶ï¼Œæ‰‹åŠ¨è§¦å‘å·¥å…·è°ƒç”¨
            if not answer.strip():
                print("âš ï¸ Agentæœªè¿”å›æœ‰æ•ˆå›ç­”ï¼Œæ‰‹åŠ¨è§¦å‘å·¥å…·è°ƒç”¨")
                if any(word in question_clean for word in ["ä¸»é¢˜", "å…³é”®è¯", "è®²äº†ä»€ä¹ˆ"]):
                    answer = pdf_meta_query_tool.invoke({})
                elif any(word in question_clean for word in ["é¡µç ", "ç¬¬å‡ é¡µ"]):
                    answer = "è¯·æ˜ç¡®è¾“å…¥è¦æŸ¥è¯¢çš„é¡µç ï¼ˆå¦‚ã€Œ3ã€ã€Œ5-8ã€ï¼‰"
                else:
                    answer = pdf_vector_retrieve_tool.invoke({"query": question_clean, "top_k": 2})
            
            sources = ["å·¥å…·è°ƒç”¨ç»“æœ"]
        except Exception as e:
            # Agentè°ƒç”¨å¤±è´¥æ—¶ï¼Œåˆ‡æ¢ä¸ºæ™®é€šRAGå…œåº•
            print(f"âš ï¸ Agentæ‰§è¡Œå¤±è´¥ï¼Œåˆ‡æ¢ä¸ºæ™®é€šRAGå›ç­”ï¼š{e}")
            retrieved_docs = retriever.invoke(question_clean)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
            sources = [f"é¡µç {doc.metadata.get('page', 'æœªçŸ¥')}" for doc in retrieved_docs] or ["æ— å¯ç”¨æ¥æº"]
            answer = answer_chain.invoke({
                "memory": combined_memory,
                "context": context,
                "question": question_clean
            }).strip()
        
        # 5. æ›´æ–°åˆ†å±‚è®°å¿†ï¼Œä¿å­˜å½“å‰å¯¹è¯
        memory_manager.add_message("user", question_clean)
        memory_manager.add_message("assistant", answer)
        
        # 6. å­˜å…¥é—®ç­”ç¼“å­˜ï¼Œåç»­å¤ç”¨
        result = (answer, sources)
        qa_cache[q_normalized] = result
        
        return result

    # ========== è¾…åŠ©å‡½æ•°ï¼ˆæ¸…ç©ºè®°å¿†/ç¼“å­˜ï¼Œæå‡æ˜“ç”¨æ€§ï¼‰ ==========
    def clear_history():
        """æ¸…ç©ºåˆ†å±‚è®°å¿†ï¼ˆçŸ­æœŸ+é•¿æœŸï¼‰"""
        return memory_manager.clear_all()
    
    def clear_all_cache():
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆé—®ç­”ç¼“å­˜+ç›¸å…³æ€§ç¼“å­˜+LLMç¼“å­˜ï¼‰"""
        global qa_cache, relevance_cache
        qa_cache = {}
        relevance_cache = {}
        llm_cache.clear()
        return "ğŸ§¹ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º"

    # è¿”å›æ ¸å¿ƒå¯è°ƒç”¨å‡½æ•°
    return qa_function, clear_history, clear_all_cache

# ------------------------------ ä¸»ç¨‹åºå…¥å£ï¼ˆç¨‹åºå¯åŠ¨å…¥å£ï¼‰ ------------------------------
if __name__ == "__main__":
    # åˆ›å»ºå¿…è¦ç›®å½•ï¼ˆç¼“å­˜ç›®å½•+å‘é‡åº“ç›®å½•ï¼‰
    os.makedirs("./cache", exist_ok=True)
    os.makedirs(CONFIG["chroma_dir"], exist_ok=True)
    
    try:
        # åˆå§‹åŒ–é—®ç­”åŠ©æ‰‹
        print("ğŸš€ åˆå§‹åŒ–PDFé—®ç­”åŠ©æ‰‹ï¼ˆåˆ†å±‚è®°å¿†+è‡ªå®šä¹‰ç¼“å­˜+å·¥å…·è°ƒç”¨ç‰ˆï¼‰...")
        qa_func, clear_history_func, clear_all_cache_func = build_qa_chain()
        print("âœ… åŠ©æ‰‹å°±ç»ªï¼")
        # æ‰“å°å‘½ä»¤è¯´æ˜
        print("ğŸ“– å‘½ä»¤è¯´æ˜ï¼š")
        print("  - quitï¼šé€€å‡ºç¨‹åº")
        print("  - clearï¼šæ¸…ç©ºå¯¹è¯è®°å¿†")
        print("  - clear_cacheï¼šæ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
        print("  - æ”¯æŒå·¥å…·è°ƒç”¨ï¼šæŸ¥ä¸»é¢˜/å…³é”®è¯ã€æŸ¥å…·ä½“å†…å®¹ã€æŒ‰é¡µç æŸ¥å†…å®¹")
        print("-" * 60)

        # å¾ªç¯æ¥æ”¶ç”¨æˆ·è¾“å…¥
        while True:
            question = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
            
            # å‘½ä»¤å¤„ç†
            if question.lower() == "quit":
                print("\nğŸ‘‹ å†è§ï¼")
                break
            elif question.lower() == "clear":
                print(f"\n{clear_history_func()}")
                continue
            elif question.lower() == "clear_cache":
                print(f"\n{clear_all_cache_func()}")
                continue
            
            # æ‰§è¡Œé—®ç­”å¹¶è¾“å‡ºç»“æœ
            answer, sources = qa_func(question)
            print(f"\nğŸ“ å›ç­”ï¼š{answer}")
            if sources:
                print(f"ğŸ“ æ¥æºï¼š{', '.join(sources)}")
            print("-" * 60)

    # å¼‚å¸¸å¤„ç†
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡ºï¼")
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯ï¼š{e}ï¼Œè¯·ç¡®ä¿PDFæ–‡ä»¶è·¯å¾„æ­£ç¡®")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{str(e)}")
        traceback.print_exc()